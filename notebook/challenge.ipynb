{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7acf6d",
   "metadata": {},
   "source": [
    "# Challenge: Edit MNIST challenge images to be correctly predicted\n",
    "\n",
    "Goal: Edit the images placed in `data/MNIST/challenge` so that the provided model predicts the correct label while keeping more than 60% of the original pixels unchanged.\n",
    "\n",
    "Description: You are given a pre-trained `SmallCNN` model and a small set of challenge images. Your task is to minimally modify these images so the model classifies them correctly. This exercise encourages you to: \n",
    "- Explore the sample dataset in `data/MNIST/sample` to understand variation and typical inputs.\n",
    "- Use explainable AI (XAI) techniques (saliency maps, Grad-CAM, Integrated Gradients, occlusion, etc.) to discover what parts of the image the model relies on.\n",
    "- Propose minimal edits (pixel changes, small masks, subtle color shifts) that change model prediction while preserving at least 60% of the original pixels.\n",
    "\n",
    "Deliverables: For each edited image, save the modified image to `data/MNIST/challenge/edited/` alongside a short report (less than 2 pages) describing the XAI insights you used and the percentage of pixels preserved. For ease of use, you have the images already in that folder and you can directly work on them. You are allowed to use any external program you want to modify the image (i.e., paint, photoshop, figma, ...).\n",
    "\n",
    "The practice can be done by more than 1 person. Final grade would depend on the number of images correctly edited (n_images_correct) with their corresponding report and number of persons working together (n_persons) following the next formula:\n",
    "$$grade = 2,5 \\times n\\_images\\_correct - 2,5 * (n\\_persons - 1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "281c72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports and device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f12e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallCNN definition (must match the trained model architecture)\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(12*7*7, 12)\n",
    "        self.fc2 = nn.Linear(12, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))  # 14x14\n",
    "        x = self.pool(x)                      # 7x7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.nn.functional.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# convenience transform\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8698f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/158852259.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_name, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=588, out_features=12, bias=True)\n",
       "  (fc2): Linear(in_features=12, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir = Path('../models')\n",
    "model_name = models_dir / 'small_cnn.pth'\n",
    "model = torch.load(model_name, map_location=device)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58b91e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_label7.png: Pred: 3, Label: 7\n",
      "1_label3.png: Pred: 1, Label: 3\n",
      "4_label2.png: Pred: 6, Label: 2\n",
      "2_label3.png: Pred: 1, Label: 3\n",
      "0_label5.png: Pred: 1, Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the images in data/MNIST/challenge/ and plot them. Label of the image is the last character of the filename.\n",
    "challenge_dir = Path('../data/MNIST/challenge')\n",
    "image_files = list(challenge_dir.glob('*.png'))\n",
    "fig, axes = plt.subplots(1, len(image_files), figsize=(12,4))\n",
    "for ax, img_file in zip(axes, image_files):\n",
    "    img = to_tensor(Image.open(img_file)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    ax.imshow(to_pil(img.squeeze().cpu()))\n",
    "    ax.set_title(f'Pred: {pred}, label: {img_file.stem[-1]}')\n",
    "    ax.axis('off')\n",
    "    print(f'{img_file.name}: Pred: {pred}, Label: {img_file.stem[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc11179",
   "metadata": {},
   "source": [
    "---\n",
    "### Any code you want to add, put it below this markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f92538",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader (lee sample/ y challenge/, etiqueta del nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "805bb1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re, torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# usa tus to_tensor / to_pil ya definidos\n",
    "def parse_label(fname: str):\n",
    "    m = re.search(r'_label(\\d+)\\.png$', fname)\n",
    "    return int(m.group(1)) if m else int(Path(fname).stem[-1])\n",
    "\n",
    "class FolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.files = sorted(Path(root_dir).glob('*.png'))\n",
    "        assert len(self.files) > 0, f\"Sin PNGs en {root_dir}\"\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.files[idx]\n",
    "        img = Image.open(p).convert('RGB').resize((28,28))\n",
    "        x = to_tensor(img)                 # [3,28,28]\n",
    "        y = parse_label(p.name)\n",
    "        return x, y, str(p)\n",
    "\n",
    "def make_loader(path, bs=64):\n",
    "    return torch.utils.data.DataLoader(FolderDataset(path), batch_size=bs, shuffle=False)\n",
    "\n",
    "dl_sample    = make_loader('../data/MNIST/sample')\n",
    "dl_challenge = make_loader('../data/MNIST/challenge/edited')\n",
    "len(dl_sample.dataset), len(dl_challenge.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f980a",
   "metadata": {},
   "source": [
    "### Inferencia en lote + DataFrame/CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2860c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample acc: 0.7196\n",
      "challenge acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_all(model, loader, device):\n",
    "    rows = []\n",
    "    for xb, yb, pb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        conf, pred = prob.max(1)\n",
    "        for i in range(xb.size(0)):\n",
    "            rows.append({\n",
    "                'path': pb[i],\n",
    "                'label': int(yb[i]),\n",
    "                'pred': int(pred[i]),\n",
    "                'conf': float(conf[i]),\n",
    "                **{f'p{k}': float(prob[i,k]) for k in range(10)}\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_sample    = predict_all(model, dl_sample, device)\n",
    "df_challenge = predict_all(model, dl_challenge, device)\n",
    "\n",
    "df_sample.to_csv('../data/MNIST/sample_preds.csv', index=False)\n",
    "df_challenge.to_csv('../data/MNIST/challenge_preds.csv', index=False)\n",
    "\n",
    "print('sample acc:', (df_sample.pred==df_sample.label).mean())\n",
    "print('challenge acc:', (df_challenge.pred==df_challenge.label).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58525a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/MNIST/sample/00000_idx0_label7.png</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.478865</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>5.567973e-04</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>5.356643e-03</td>\n",
       "      <td>0.021750</td>\n",
       "      <td>9.217982e-03</td>\n",
       "      <td>0.046576</td>\n",
       "      <td>4.323789e-01</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>4.788652e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/MNIST/sample/00001_idx1_label2.png</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>8.480680e-12</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>8.557539e-11</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>1.136868e-08</td>\n",
       "      <td>0.146423</td>\n",
       "      <td>7.702062e-13</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>7.426890e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/MNIST/sample/00002_idx2_label1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0.016178</td>\n",
       "      <td>2.970071e-01</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>2.589439e-01</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>2.226218e-01</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>9.445409e-02</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>5.793344e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/MNIST/sample/00003_idx3_label0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941117</td>\n",
       "      <td>0.941117</td>\n",
       "      <td>9.154581e-11</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>4.081886e-11</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>2.078366e-08</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>7.595002e-11</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>1.689984e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/MNIST/sample/00004_idx4_label4.png</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985357</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>4.537991e-17</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>2.044186e-16</td>\n",
       "      <td>0.985357</td>\n",
       "      <td>4.044026e-13</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>7.268429e-14</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>7.813033e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path  label  pred      conf  \\\n",
       "0  ../data/MNIST/sample/00000_idx0_label7.png      7     9  0.478865   \n",
       "1  ../data/MNIST/sample/00001_idx1_label2.png      2     2  0.841987   \n",
       "2  ../data/MNIST/sample/00002_idx2_label1.png      1     1  0.297007   \n",
       "3  ../data/MNIST/sample/00003_idx3_label0.png      0     0  0.941117   \n",
       "4  ../data/MNIST/sample/00004_idx4_label4.png      4     4  0.985357   \n",
       "\n",
       "         p0            p1        p2            p3        p4            p5  \\\n",
       "0  0.000255  5.567973e-04  0.000373  5.356643e-03  0.021750  9.217982e-03   \n",
       "1  0.004476  8.480680e-12  0.841987  8.557539e-11  0.001008  1.136868e-08   \n",
       "2  0.016178  2.970071e-01  0.009208  2.589439e-01  0.007789  2.226218e-01   \n",
       "3  0.941117  9.154581e-11  0.006714  4.081886e-11  0.001095  2.078366e-08   \n",
       "4  0.000033  4.537991e-17  0.000202  2.044186e-16  0.985357  4.044026e-13   \n",
       "\n",
       "         p6            p7        p8            p9  \n",
       "0  0.046576  4.323789e-01  0.004669  4.788652e-01  \n",
       "1  0.146423  7.702062e-13  0.006107  7.426890e-13  \n",
       "2  0.015281  9.445409e-02  0.020584  5.793344e-02  \n",
       "3  0.049775  7.595002e-11  0.001299  1.689984e-11  \n",
       "4  0.002034  7.268429e-14  0.012375  7.813033e-16  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "001ab7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/MNIST/challenge/edited/0_label5.png</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0.016178</td>\n",
       "      <td>2.970071e-01</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>2.589439e-01</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>2.226218e-01</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>9.445409e-02</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>5.793344e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/MNIST/challenge/edited/1_label3.png</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0.016178</td>\n",
       "      <td>2.970071e-01</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>2.589439e-01</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>2.226218e-01</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>9.445409e-02</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>5.793344e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/MNIST/challenge/edited/2_label3.png</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0.016178</td>\n",
       "      <td>2.970071e-01</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>2.589439e-01</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>2.226218e-01</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>9.445409e-02</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>5.793344e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/MNIST/challenge/edited/3_label7.png</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.231726</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>1.993954e-01</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>2.317263e-01</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>2.164780e-01</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>1.613207e-01</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>1.061987e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/MNIST/challenge/edited/4_label2.png</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.455016</td>\n",
       "      <td>0.108265</td>\n",
       "      <td>4.519096e-12</td>\n",
       "      <td>0.434832</td>\n",
       "      <td>8.589031e-12</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>6.445534e-09</td>\n",
       "      <td>0.455016</td>\n",
       "      <td>3.178385e-13</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>5.347470e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          path  label  pred      conf  \\\n",
       "0  ../data/MNIST/challenge/edited/0_label5.png      5     1  0.297007   \n",
       "1  ../data/MNIST/challenge/edited/1_label3.png      3     1  0.297007   \n",
       "2  ../data/MNIST/challenge/edited/2_label3.png      3     1  0.297007   \n",
       "3  ../data/MNIST/challenge/edited/3_label7.png      7     3  0.231726   \n",
       "4  ../data/MNIST/challenge/edited/4_label2.png      2     6  0.455016   \n",
       "\n",
       "         p0            p1        p2            p3        p4            p5  \\\n",
       "0  0.016178  2.970071e-01  0.009208  2.589439e-01  0.007789  2.226218e-01   \n",
       "1  0.016178  2.970071e-01  0.009208  2.589439e-01  0.007789  2.226218e-01   \n",
       "2  0.016178  2.970071e-01  0.009208  2.589439e-01  0.007789  2.226218e-01   \n",
       "3  0.014017  1.993954e-01  0.008931  2.317263e-01  0.012538  2.164780e-01   \n",
       "4  0.108265  4.519096e-12  0.434832  8.589031e-12  0.000618  6.445534e-09   \n",
       "\n",
       "         p6            p7        p8            p9  \n",
       "0  0.015281  9.445409e-02  0.020584  5.793344e-02  \n",
       "1  0.015281  9.445409e-02  0.020584  5.793344e-02  \n",
       "2  0.015281  9.445409e-02  0.020584  5.793344e-02  \n",
       "3  0.024859  1.613207e-01  0.024536  1.061987e-01  \n",
       "4  0.455016  3.178385e-13  0.001268  5.347470e-13  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_challenge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a026c6",
   "metadata": {},
   "source": [
    "### Técnicas de XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23783942",
   "metadata": {},
   "source": [
    "### GRADCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce2d05b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooks limpiados correctamente ✅\n"
     ]
    }
   ],
   "source": [
    "# cierra gradcam viejo si existe\n",
    "try:\n",
    "    gradcam.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# elimina TODOS los hooks de todos los submódulos (forward/backward)\n",
    "def _clear_all_hooks(module):\n",
    "    for m in module.modules():\n",
    "        for attr in (\"_forward_hooks\", \"_forward_pre_hooks\", \"_backward_hooks\"):\n",
    "            d = getattr(m, attr, None)\n",
    "            if isinstance(d, dict):\n",
    "                d.clear()\n",
    "\n",
    "_clear_all_hooks(model)\n",
    "print(\"hooks limpiados correctamente ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5ba9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradCAM reinstanciado correctamente ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM_SAFE:\n",
    "    def __init__(self, model, layer_module):\n",
    "        self.model = model.eval()\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "\n",
    "        def fwd_hook(_, __, out):\n",
    "            self.activations = out\n",
    "            if out.requires_grad:\n",
    "                def bwd_hook(grad):\n",
    "                    self.gradients = grad\n",
    "                out.register_hook(bwd_hook)\n",
    "            else:\n",
    "                self.gradients = None  # forward sin gradientes\n",
    "\n",
    "        self.h = layer_module.register_forward_hook(fwd_hook)\n",
    "\n",
    "    def __call__(self, x, target):\n",
    "        x = x.unsqueeze(0) if x.dim()==3 else x\n",
    "        x = x.to(device).detach().requires_grad_(True)\n",
    "        logits = self.model(x)\n",
    "        score = logits[0, target]\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        score.backward()\n",
    "\n",
    "        A, dA = self.activations, self.gradients\n",
    "        assert (A is not None) and (dA is not None), \"No hay gradientes/activaciones (¿forward bajo no_grad?).\"\n",
    "\n",
    "        w = dA.mean(dim=(2,3), keepdim=True)\n",
    "        cam = torch.relu((w * A).sum(1, keepdim=True))\n",
    "        cam = F.interpolate(cam, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)[0,0]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam.detach().cpu()\n",
    "\n",
    "    def close(self):\n",
    "        self.h.remove()\n",
    "\n",
    "# crear nueva instancia del GradCAM con tu modelo\n",
    "gradcam = GradCAM_SAFE(model, model.conv1)\n",
    "print(\"GradCAM reinstanciado correctamente ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5628c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/2929275833.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardado en ../data/MNIST/gradcam/sample_xai\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, cv2, torch\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "from matplotlib import cm\n",
    "\n",
    "def colorize_cam(cam_01):\n",
    "    # cam_01 -> np.float32 [H,W] en 0..1, luego aplica 'jet' y quita alfa\n",
    "    if isinstance(cam_01, torch.Tensor):\n",
    "        cam = cam_01.detach().cpu().squeeze().float().numpy()\n",
    "    else:\n",
    "        cam = np.array(cam_01, dtype=np.float32)\n",
    "    cam = np.nan_to_num(cam, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    cam = np.clip(cam, 0.0, 1.0).astype(np.float32)\n",
    "    jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n",
    "    return jet.astype(np.float32)\n",
    "\n",
    "def overlay_color(x01, cam01, alpha=0.55):\n",
    "    base = x01.detach().cpu().permute(1,2,0).float().numpy()      # (H,W,3) 0..1\n",
    "    base = np.clip(base, 0.0, 1.0).astype(np.float32)\n",
    "    heat = colorize_cam(cam01)                                     # (H,W,3) 0..1\n",
    "    out  = np.clip((1 - alpha) * base + alpha * heat, 0.0, 1.0)\n",
    "    return torch.from_numpy(out).permute(2,0,1) \n",
    "\n",
    "def save_xai_split(split_loader, split_name, target=\"true\"):\n",
    "    \"\"\"\n",
    "    target: \"true\" (etiqueta) o \"pred\" (clase predicha)\n",
    "    \"\"\"\n",
    "    out_dir = Path(f'../data/MNIST/gradcam/{split_name}_xai')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for xb, yb, pb in split_loader:\n",
    "        with torch.no_grad():\n",
    "            preds = model(xb.to(device)).argmax(1).cpu()\n",
    "\n",
    "        for i in range(xb.size(0)):\n",
    "            x = xb[i]\n",
    "            y = int(yb[i])\n",
    "            pred = int(preds[i])\n",
    "            tgt = y if target==\"true\" else pred\n",
    "\n",
    "            cam = gradcam(x, tgt)          # torch[H,W] en 0..1\n",
    "            ov  = overlay_color(x.cpu(), cam, alpha=0.55)\n",
    "\n",
    "            stem = Path(pb[i]).stem\n",
    "            suffix = \"true\" if target==\"true\" else \"pred\"\n",
    "            save_image(ov, out_dir/f'{stem}_cam_{suffix}.png')\n",
    "\n",
    "    print('guardado en', out_dir)\n",
    "\n",
    "# ejemplos:\n",
    "save_xai_split(dl_sample, 'sample', target=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "11f3ee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardado en ../data/MNIST/gradcam/challenge_xai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/3085249586.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, cv2, torch\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "from matplotlib import cm\n",
    "\n",
    "def colorize_cam(cam_01):\n",
    "    # cam_01 -> np.float32 [H,W] en 0..1, luego aplica 'jet' y quita alfa\n",
    "    if isinstance(cam_01, torch.Tensor):\n",
    "        cam = cam_01.detach().cpu().squeeze().float().numpy()\n",
    "    else:\n",
    "        cam = np.array(cam_01, dtype=np.float32)\n",
    "    cam = np.nan_to_num(cam, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    cam = np.clip(cam, 0.0, 1.0).astype(np.float32)\n",
    "    jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n",
    "    return jet.astype(np.float32)\n",
    "\n",
    "def overlay_color(x01, cam01, alpha=0.55):\n",
    "    base = x01.detach().cpu().permute(1,2,0).float().numpy()      # (H,W,3) 0..1\n",
    "    base = np.clip(base, 0.0, 1.0).astype(np.float32)\n",
    "    heat = colorize_cam(cam01)                                     # (H,W,3) 0..1\n",
    "    out  = np.clip((1 - alpha) * base + alpha * heat, 0.0, 1.0)\n",
    "    return torch.from_numpy(out).permute(2,0,1) \n",
    "\n",
    "def save_xai_split(split_loader, split_name, target=\"true\"):\n",
    "    \"\"\"\n",
    "    target: \"true\" (etiqueta) o \"pred\" (clase predicha)\n",
    "    \"\"\"\n",
    "    out_dir = Path(f'../data/MNIST/gradcam/{split_name}_xai')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for xb, yb, pb in split_loader:\n",
    "        with torch.no_grad():\n",
    "            preds = model(xb.to(device)).argmax(1).cpu()\n",
    "\n",
    "        for i in range(xb.size(0)):\n",
    "            x = xb[i]\n",
    "            y = int(yb[i])\n",
    "            pred = int(preds[i])\n",
    "            tgt = y if target==\"true\" else pred\n",
    "\n",
    "            cam = gradcam(x, tgt)          # torch[H,W] en 0..1\n",
    "            ov  = overlay_color(x.cpu(), cam, alpha=0.55)\n",
    "\n",
    "            stem = Path(pb[i]).stem\n",
    "            suffix = \"true\" if target==\"true\" else \"pred\"\n",
    "            save_image(ov, out_dir/f'{stem}_cam_{suffix}.png')\n",
    "\n",
    "    print('guardado en', out_dir)\n",
    "\n",
    "# ejemplos:\n",
    "save_xai_split(dl_challenge, 'challenge', target=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c7b42",
   "metadata": {},
   "source": [
    "### INTEGRATED GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f69e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_baseline(x, kind=\"black\"):\n",
    "    if kind == \"black\":  return torch.zeros_like(x)\n",
    "    if kind == \"white\":  return torch.ones_like(x)\n",
    "    if kind == \"blur\":\n",
    "        k, pad = 5, 2\n",
    "        xx = x.unsqueeze(0)\n",
    "        ker = torch.ones(1,1,k,k, device=xx.device) / (k*k)\n",
    "        ch = [F.conv2d(xx[:,c:c+1], ker, padding=pad) for c in range(xx.size(1))]\n",
    "        return torch.cat(ch, dim=1).squeeze(0).clamp(0,1)\n",
    "    raise ValueError(kind)\n",
    "\n",
    "def integrated_gradients(model, x, target, steps=50, baseline_kind=\"black\", signed=False, device=None):\n",
    "    device = device or next(model.parameters()).device\n",
    "    x = x.to(device)\n",
    "    baseline = make_baseline(x, baseline_kind).to(device)\n",
    "\n",
    "    grads_sum = torch.zeros_like(x)\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    for i in range(1, steps+1):\n",
    "        alpha = i/steps\n",
    "        xi = (baseline + alpha*(x - baseline)).unsqueeze(0)   # [1,3,H,W]\n",
    "        xi.requires_grad_(True)\n",
    "        logits = model(xi)\n",
    "        score  = logits[0, target]\n",
    "        # grad w.r.t. input directly (robusto; no depende de xi.grad)\n",
    "        grad = torch.autograd.grad(score, xi, retain_graph=False, create_graph=False, allow_unused=True)[0]\n",
    "        if grad is None:\n",
    "            raise RuntimeError(\"autograd.grad devolvió None; revisa que no estés en torch.inference_mode()\")\n",
    "        grads_sum += grad.squeeze(0)\n",
    "\n",
    "    avg_grad = grads_sum / steps\n",
    "    ig = (x - baseline) * avg_grad   # [3,H,W]\n",
    "    if not signed:\n",
    "        ig = ig.abs()\n",
    "    ig = ig.max(dim=0, keepdim=False).values      # [H,W]\n",
    "    ig = ig - ig.min()\n",
    "    ig = ig / (ig.max() + 1e-8)\n",
    "    return ig.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "341e2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "\n",
    "def save_ig_split(split_loader, split_name, target=\"true\", steps=50, baseline_kind=\"black\", signed=False):\n",
    "    \"\"\"\n",
    "    target: \"true\" (etiqueta) o \"pred\" (clase predicha)\n",
    "    \"\"\"\n",
    "    out_dir = Path(f'../data/MNIST/integrated_gradients/{split_name}_xai')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for xb, yb, pb in split_loader:\n",
    "        # predicciones en batch para target=\"pred\"\n",
    "        with torch.no_grad():\n",
    "            preds = model(xb.to(device)).argmax(1).cpu()\n",
    "\n",
    "        for i in range(xb.size(0)):\n",
    "            x = xb[i]\n",
    "            y = int(yb[i])\n",
    "            pred = int(preds[i])\n",
    "            tgt = y if target == \"true\" else pred\n",
    "\n",
    "            ig_map = integrated_gradients(model, x, tgt, steps=steps, baseline_kind=baseline_kind, signed=signed, device=device)\n",
    "            ov    = overlay_color(x.cpu(), ig_map, alpha=0.55)  # usa tu overlay en color existente\n",
    "\n",
    "            stem = Path(pb[i]).stem\n",
    "            suffix = f\"ig_{target}_s{steps}_{baseline_kind}{'_signed' if signed else ''}.png\"\n",
    "            save_image(ov, out_dir / f\"{stem}_{suffix}\")\n",
    "\n",
    "    print(\"IG guardado en\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25f324af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG guardado en ../data/MNIST/integrated_gradients/edited_ig_xai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/3085249586.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de ejecución:\n",
    "save_ig_split(dl_challenge, 'edited_ig', target=\"true\", steps=32, baseline_kind=\"black\", signed=False)\n",
    "# save_ig_split(dl_sample, 'sample', target=\"pred\", steps=32, baseline_kind=\"blur\", signed=False)\n",
    "# save_ig_split(dl_sample, 'sample', target=\"true\", steps=64, baseline_kind=\"white\", signed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0bd1262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/3085249586.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG guardado en ../data/MNIST/integrated_gradients/challenge_xai\n"
     ]
    }
   ],
   "source": [
    "save_ig_split(dl_challenge, 'challenge', target=\"true\", steps=32, baseline_kind=\"black\", signed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3072f2d",
   "metadata": {},
   "source": [
    "# SMOOTHGRAD\n",
    "#### Calcula el mapa de importancia (gradiente suavizado) para una clase concreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e3573cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad(model, x, y, n, noise):\n",
    "    grads = torch.zeros_like(x)\n",
    "    for _ in range(n):\n",
    "        noise_tensor = torch.randn_like(x) * noise\n",
    "        x_noisy = (x + noise_tensor).clamp(0,1)\n",
    "        x_noisy.requires_grad_(True)\n",
    "        out = model(x_noisy)\n",
    "        score = out[0,y]\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        score.backward()\n",
    "        grads += x_noisy.grad.detach()\n",
    "    grads = grads.abs().mean(1, keepdim=True)\n",
    "    grads = grads[0] / grads.max()\n",
    "    return grads.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d80c4f",
   "metadata": {},
   "source": [
    "#### Crea dos carpetas con los mapas de calor de cada imagen defectuosa (challenge):\n",
    "#### smooth_grad_predicted_label → mapa para la clase predicha\n",
    "#### smooth_grad_true_label → mapa para la clase verdadera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b013e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def smooth_grad_predicted_label(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((28,28))\n",
    "    x = to_tensor(img).unsqueeze(0).to(device)   # [1,3,28,28]\n",
    "    y_true = int(Path(img_path).stem[-1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = int(model(x).argmax(1).item())\n",
    "\n",
    "    #tgt = y_true if use_target==\"true\" else y_pred\n",
    "    tgt = y_pred\n",
    "\n",
    "    sg_map = smoothgrad(model, x.clone(), tgt, n=125, noise=0.3)\n",
    "\n",
    "    ov_sg  = overlay_color(x.squeeze(0).cpu(), torch.as_tensor(sg_map),      alpha=0.55)\n",
    "\n",
    "    out_dir = Path(\"../data/MNIST/smooth_grad_predicted_label\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stem = Path(img_path).stem\n",
    "    from torchvision.utils import save_image\n",
    "    save_image(ov_sg,  out_dir/f\"{stem}_SG_predicted_label.png\")\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "477abb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/3085249586.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_label5.png pred= 1 true= 5\n",
      "1_label3.png pred= 1 true= 3\n",
      "2_label3.png pred= 1 true= 3\n",
      "3_label7.png pred= 3 true= 7\n",
      "4_label2.png pred= 6 true= 2\n",
      "Listo ✅  (archivos en ../data/MNIST/smooth_grad_predicted_label\n"
     ]
    }
   ],
   "source": [
    "challenge_dir = Path(\"../data/MNIST/challenge\")\n",
    "img_paths = sorted(challenge_dir.glob(\"*.png\"))\n",
    "for p in img_paths:\n",
    "    yt, yp = smooth_grad_predicted_label(\n",
    "        p)\n",
    "    print(p.name, \"pred=\", yp, \"true=\", yt)\n",
    "print(\"Listo ✅  (archivos en ../data/MNIST/smooth_grad_predicted_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ec023759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_grad_true_label(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((28,28))\n",
    "    x = to_tensor(img).unsqueeze(0).to(device)   # [1,3,28,28]\n",
    "    y_true = int(Path(img_path).stem[-1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = int(model(x).argmax(1).item())\n",
    "\n",
    "    #tgt = y_true if use_target==\"true\" else y_pred\n",
    "    tgt = y_true\n",
    "\n",
    "    sg_map = smoothgrad(model, x.clone(), tgt, n=125, noise=0.3)\n",
    "\n",
    "    ov_sg  = overlay_color(x.squeeze(0).cpu(), torch.as_tensor(sg_map), alpha=0.55)\n",
    "\n",
    "    out_dir = Path(\"../data/MNIST/smooth_grad_true_label\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stem = Path(img_path).stem\n",
    "    from torchvision.utils import save_image\n",
    "    save_image(ov_sg,  out_dir/f\"{stem}_SG_true_label.png\")\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f049443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/lsf3fbc14ldfy85pcnmhcm3m0000gn/T/ipykernel_27206/3085249586.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = cm.get_cmap('jet')(cam)[:, :, :3]        # (H,W,3) RGB 0..1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_label5.png pred= 1 true= 5\n",
      "1_label3.png pred= 1 true= 3\n",
      "2_label3.png pred= 1 true= 3\n",
      "3_label7.png pred= 3 true= 7\n",
      "4_label2.png pred= 6 true= 2\n",
      "Listo ✅  (archivos en ../data/MNIST/smooth_grad_true_label\n"
     ]
    }
   ],
   "source": [
    "challenge_dir = Path(\"../data/MNIST/challenge\")\n",
    "img_paths = sorted(challenge_dir.glob(\"*.png\"))\n",
    "for p in img_paths:\n",
    "    yt, yp = smooth_grad_true_label(\n",
    "        p)\n",
    "    print(p.name, \"pred=\", yp, \"true=\", yt)\n",
    "print(\"Listo ✅  (archivos en ../data/MNIST/smooth_grad_true_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0fc9e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 2: 20 máscaras -> promedio guardado.\n",
      "Clase 3: 20 máscaras -> promedio guardado.\n",
      "Clase 5: 0 ejemplos válidos (no se genera promedio).\n",
      "Clase 7: 20 máscaras -> promedio guardado.\n",
      "Clase 5: 20 máscaras -> promedio guardado en ../data/MNIST/avg_masks_per_class\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def _topk_mask_np(heat_01: np.ndarray, pct: float):\n",
    "    H, W = heat_01.shape\n",
    "    k = max(1, int(round(pct * H * W)))\n",
    "    flat = heat_01.reshape(-1)\n",
    "    idx = np.argpartition(-flat, k-1)[:k]\n",
    "    mask = np.zeros_like(flat, dtype=bool)\n",
    "    mask[idx] = True\n",
    "    return mask.reshape(H, W)\n",
    "\n",
    "def _edit_with_masks(x3: torch.Tensor, mask_remove: np.ndarray, mask_add: np.ndarray):\n",
    "    mr = torch.from_numpy(mask_remove).to(x3.device)\n",
    "    ma = torch.from_numpy(mask_add).to(x3.device)\n",
    "    x_ed = x3.clone()\n",
    "    x_ed[:, mr] = 0.0\n",
    "    x_ed[:, ma] = 1.0\n",
    "    changed = (mr | ma).float().mean().item()\n",
    "    return x_ed, changed\n",
    "\n",
    "@torch.no_grad()\n",
    "def _pred_label(x3: torch.Tensor) -> int:\n",
    "    return int(model(x3.unsqueeze(0).to(device)).argmax(1).item())\n",
    "\n",
    "# --- crear máscara promedio por clase usando las imágenes correctas del sample ---\n",
    "sample_dir = Path(\"../data/MNIST/sample\")\n",
    "mask_dir = Path(\"../data/MNIST/sample_sg_masks\"); mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def generate_avg_masks_from_sample_subset(\n",
    "    target_labels=(2,3,5,7),\n",
    "    max_per_class=20,      # tope de imágenes correctas por clase\n",
    "    pct=0.15,              # top-% para la máscara binaria\n",
    "    n_sg=64,               # menos iteraciones para ir rápido (antes 125)\n",
    "    noise_sg=0.30,\n",
    "    shuffle=True,          # opcional: barajar para muestreo aleatorio\n",
    "    seed=42\n",
    "):\n",
    "    target_labels = set(map(int, target_labels))\n",
    "    class_masks = {c: [] for c in target_labels}\n",
    "    counts = {c: 0 for c in target_labels}\n",
    "\n",
    "    files = sorted(sample_dir.glob(\"*.png\"))\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        files = list(files)\n",
    "        rng.shuffle(files)\n",
    "\n",
    "    for p in files:\n",
    "        label = int(p.stem[-1])\n",
    "        if label not in target_labels:\n",
    "            continue\n",
    "        if counts[label] >= max_per_class:\n",
    "            continue\n",
    "\n",
    "        img = Image.open(p).convert(\"RGB\").resize((28,28))\n",
    "        x = to_tensor(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # solo ejemplos BIEN clasificados\n",
    "        with torch.no_grad():\n",
    "            y_pred = int(model(x).argmax(1).item())\n",
    "        if y_pred != label:\n",
    "            continue\n",
    "\n",
    "        # SmoothGrad (rápido)\n",
    "        sg_map = smoothgrad(model, x.clone(), label, n_sg, noise_sg)  # np[H,W] en 0..1\n",
    "        mask = _topk_mask_np(sg_map, pct).astype(np.float32)          # 0/1\n",
    "        class_masks[label].append(mask)\n",
    "        counts[label] += 1\n",
    "\n",
    "        # corta si ya has llenado todas\n",
    "        if all(counts[c] >= max_per_class for c in target_labels):\n",
    "            break\n",
    "\n",
    "    # promedio por clase\n",
    "    avg_masks = {}\n",
    "    out_dir = Path(\"../data/MNIST/avg_masks_per_class\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for c in target_labels:\n",
    "        if class_masks[c]:\n",
    "            avg = np.mean(np.stack(class_masks[c], axis=0), axis=0)   # en [0,1]\n",
    "            avg_masks[c] = avg / (avg.max() + 1e-8)\n",
    "            np.save(out_dir / f\"avg_mask_{c}.npy\", avg_masks[c])\n",
    "            Image.fromarray((avg_masks[c]*255).astype(np.uint8)).save(out_dir / f\"avg_mask_{c}.png\")\n",
    "            print(f\"Clase {c}: {len(class_masks[c])} máscaras -> promedio guardado.\")\n",
    "        else:\n",
    "            print(f\"Clase {c}: 0 ejemplos válidos (no se genera promedio).\")\n",
    "    return avg_masks\n",
    "\n",
    "# Ejecuta (solo 2,3,5,7; 20 por clase; n=64)\n",
    "avg_masks = generate_avg_masks_from_sample_subset(\n",
    "    target_labels=(2,3,5,7),\n",
    "    max_per_class=20,\n",
    "    pct=0.15,\n",
    "    n_sg=64,\n",
    "    noise_sg=0.30,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "avg_mask_dir = Path(\"../data/MNIST/avg_masks_per_class\"); avg_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_avg_mask_for_5_fallback(\n",
    "    max_per_class=20,     # cuántas imágenes usar como máximo\n",
    "    pct=0.15,             # porcentaje top para la máscara binaria\n",
    "    n_sg=64,              # menos iteraciones para ir rápido\n",
    "    noise_sg=0.30\n",
    "):\n",
    "    files = sorted(sample_dir.glob(\"*.png\"))\n",
    "\n",
    "    # 1) candidatos donde el modelo PREDICE 5\n",
    "    pred5 = []\n",
    "    probs5 = []\n",
    "    for p in files:\n",
    "        img = Image.open(p).convert(\"RGB\").resize((28,28))\n",
    "        x = to_tensor(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            y_pred = int(logits.argmax(1).item())\n",
    "            prob5  = torch.softmax(logits, dim=1)[0,5].item()\n",
    "        if y_pred == 5:\n",
    "            pred5.append(p)\n",
    "            probs5.append(prob5)\n",
    "\n",
    "    # si no hay suficientes, 2) coge top-prob clase 5\n",
    "    if len(pred5) < max_per_class:\n",
    "        # ordena todas por prob5 desc y añade hasta completar\n",
    "        all_scores = []\n",
    "        for p in files:\n",
    "            img = Image.open(p).convert(\"RGB\").resize((28,28))\n",
    "            x = to_tensor(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                prob5 = torch.softmax(model(x), dim=1)[0,5].item()\n",
    "            all_scores.append((prob5, p))\n",
    "        all_scores.sort(reverse=True, key=lambda t: t[0])\n",
    "        for _, p in all_scores:\n",
    "            if p in pred5: \n",
    "                continue\n",
    "            pred5.append(p)\n",
    "            if len(pred5) >= max_per_class:\n",
    "                break\n",
    "\n",
    "    if len(pred5) == 0:\n",
    "        print(\"No se encontraron candidatos para clase 5.\")\n",
    "        return None\n",
    "\n",
    "    # 3) SG wrt clase 5 -> máscara top-pct\n",
    "    masks = []\n",
    "    for p in pred5[:max_per_class]:\n",
    "        img = Image.open(p).convert(\"RGB\").resize((28,28))\n",
    "        x = to_tensor(img).unsqueeze(0).to(device)\n",
    "        sg_map = smoothgrad(model, x.clone(), 5, n_sg, noise_sg)  # np[H,W] en 0..1\n",
    "        mask = _topk_mask_np(sg_map, pct).astype(np.float32)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # 4) promedio y guardado\n",
    "    avg = np.mean(np.stack(masks, axis=0), axis=0)\n",
    "    avg = avg / (avg.max() + 1e-8)\n",
    "    np.save(avg_mask_dir / \"avg_mask_5.npy\", avg)\n",
    "    Image.fromarray((avg*255).astype(np.uint8)).save(avg_mask_dir / \"avg_mask_5.png\")\n",
    "    print(f\"Clase 5: {len(masks)} máscaras -> promedio guardado en {avg_mask_dir}\")\n",
    "    return avg\n",
    "\n",
    "# Ejecuta el parche\n",
    "avg5 = build_avg_mask_for_5_fallback(max_per_class=20, pct=0.15, n_sg=64, noise_sg=0.30)\n",
    "\n",
    "# opcional: inserta/actualiza en tu dict avg_masks si lo estás usando aguas abajo\n",
    "try:\n",
    "    avg_masks\n",
    "except NameError:\n",
    "    avg_masks = {}\n",
    "if avg5 is not None:\n",
    "    avg_masks[5] = avg5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ea35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_label5.png: 1→9 | edit=40.8%\n",
      "1_label3.png: 1→7 | edit=43.6%\n",
      "2_label3.png: 1→1 | edit=39.3%\n",
      "3_label7.png: 3→9 | edit=41.3%\n",
      "4_label2.png: 6→8 | edit=42.7%\n",
      "Imágenes editadas en ../data/MNIST/challenge/edited\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def edit_image_sg_with_avg(img_path,\n",
    "                           remove_pct=0.15,\n",
    "                           add_pct=0.15,\n",
    "                           max_budget=0.40):\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((28,28))\n",
    "    x = to_tensor(img).to(device)\n",
    "    y_true = int(Path(img_path).stem[-1])\n",
    "    y_pred = _pred_label(x)\n",
    "\n",
    "    sg_pred = smoothgrad(model, x.unsqueeze(0).clone(), y_pred, n=125, noise=0.3)\n",
    "    sg_true = smoothgrad(model, x.unsqueeze(0).clone(), y_true, n=125, noise=0.3)\n",
    "\n",
    "    # máscara de la predicha (negro)\n",
    "    m_remove = _topk_mask_np(sg_pred, remove_pct)\n",
    "\n",
    "    # máscara de la verdadera (blanco)\n",
    "    m_add = _topk_mask_np(sg_true, add_pct).astype(np.float32)\n",
    "\n",
    "    # si hay máscara promedio de esa clase, la añadimos\n",
    "    if y_true in avg_masks:\n",
    "        m_add = np.clip(m_add + avg_masks[y_true], 0, 1)\n",
    "\n",
    "    # controlar presupuesto máximo\n",
    "    union_frac = (m_remove | (m_add>0.5)).mean()\n",
    "    if union_frac > max_budget:\n",
    "        scale = max_budget / (union_frac + 1e-8)\n",
    "        m_remove = _topk_mask_np(sg_pred, remove_pct*scale)\n",
    "        m_add = _topk_mask_np(sg_true, add_pct*scale)\n",
    "\n",
    "    x_edit, changed_frac = _edit_with_masks(x, m_remove, m_add>0.5)\n",
    "    new_pred = _pred_label(x_edit)\n",
    "\n",
    "    out_dir = Path(\"../data/MNIST/challenge/edited\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_image(x_edit.cpu(), out_dir/f\"{Path(img_path).stem}_edited.png\")\n",
    "\n",
    "    return {\n",
    "        \"file\": Path(img_path).stem,\n",
    "        \"true\": y_true,\n",
    "        \"pred_before\": y_pred,\n",
    "        \"pred_after\": new_pred,\n",
    "        \"changed_frac\": changed_frac\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for p in sorted(challenge_dir.glob(\"*.png\")):\n",
    "    r = edit_image_sg_with_avg(p)\n",
    "    rows.append(r)\n",
    "    print(f\"{p.name}: {r['pred_before']}→{r['pred_after']} | edit={r['changed_frac']*100:.1f}%\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"../data/MNIST/challenge/edited_summary.csv\", index=False)\n",
    "print(\"Imágenes editadas en ../data/MNIST/challenge/edited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7992207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_label5.png: 1→9  changed=18.8%\n",
      "1_label3.png: 1→5  changed=27.9%\n",
      "2_label3.png: 1→1  changed=17.1%\n",
      "3_label7.png: 3→7  changed=18.9%\n",
      "4_label2.png: 6→8  changed=21.7%\n",
      "\n",
      "Imágenes editadas en: ../data/MNIST/challenge/edited\n",
      "Resumen CSV en:      ../data/MNIST/challenge/edited_summary.csv\n",
      "Accuracy antes: 0.0 después: 0.2\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# challenge_dir = Path(\"../data/MNIST/challenge\")\n",
    "# rows = []\n",
    "# for p in sorted(challenge_dir.glob(\"*.png\")):\n",
    "#     r = edit_image_sg(\n",
    "#         p,\n",
    "#         remove_pct=0.15,   # píxeles “predicha” que quitamos (negro)\n",
    "#         add_pct=0.15,      # píxeles “verdadera” que añadimos (blanco)\n",
    "#         max_budget=0.40,   # garantiza ≥60% sin cambiar\n",
    "#         n=125, noise=0.3\n",
    "#     )\n",
    "#     rows.append(r)\n",
    "#     print(f\"{p.name}: {r['pred_before']}→{r['pred_after']}  changed={r['changed_frac']*100:.1f}%\")\n",
    "\n",
    "# df = pd.DataFrame(rows)\n",
    "# df.to_csv(\"../data/MNIST/challenge/edited_summary.csv\", index=False)\n",
    "# print(\"\\nImágenes editadas en: ../data/MNIST/challenge/edited\")\n",
    "# print(\"Resumen CSV en:      ../data/MNIST/challenge/edited_summary.csv\")\n",
    "# print(\"Accuracy antes:\", (df.pred_before==df.true).mean(), \"después:\", (df.pred_after==df.true).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81670f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge dir: ../data/MNIST/challenge\n",
      "Edited dir: ../data/MNIST/challenge/edited\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# # ruta base del dataset challenge\n",
    "# challenge_dir = Path(\"../data/MNIST/challenge\")\n",
    "\n",
    "# # carpeta donde guardaste tus imágenes modificadas\n",
    "# edited_dir = challenge_dir / \"edited\"\n",
    "\n",
    "# print(\"Challenge dir:\", challenge_dir)\n",
    "# print(\"Edited dir:\", edited_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d9167c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "challenge_dir = Path(\"../data/MNIST/challenge\")\n",
    "edited_dir    = challenge_dir / \"edited\"\n",
    "\n",
    "# Si tus archivos actuales tienen sufijo \"_edited.png\", renómbralos a su nombre original.\n",
    "for orig in sorted(challenge_dir.glob(\"*.png\")):\n",
    "    stem = orig.stem  # p.ej. \"0_label5\"\n",
    "    cand = edited_dir / f\"{stem}_edited.png\"\n",
    "    if cand.exists():\n",
    "        target = edited_dir / f\"{stem}.png\"\n",
    "        if target.exists():\n",
    "            target.unlink()\n",
    "        shutil.move(str(cand), str(target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670384a",
   "metadata": {},
   "source": [
    "---\n",
    "## Check if you have passed the challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "039eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of editing an image\n",
    "def calculate_edit_percentage(original_img, edited_img):\n",
    "    original_pixels = original_img.load()\n",
    "    edited_pixels = edited_img.load()\n",
    "    width, height = original_img.size\n",
    "    total_pixels = width * height\n",
    "    changed_pixels = 0\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if original_pixels[x, y] != edited_pixels[x, y]:\n",
    "                changed_pixels += 1\n",
    "\n",
    "    return (changed_pixels / total_pixels) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "da710de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edited 3_label7.png: Pred: 9, Label: 7, correct: False\n",
      "Edit Percentage: 41.07%\n",
      "Edited 1_label3.png: Pred: 7, Label: 3, correct: False\n",
      "Edit Percentage: 43.62%\n",
      "Edited 4_label2.png: Pred: 8, Label: 2, correct: False\n",
      "Edit Percentage: 42.73%\n",
      "Edited 2_label3.png: Pred: 1, Label: 3, correct: False\n",
      "Edit Percentage: 39.16%\n",
      "Edited 0_label5.png: Pred: 9, Label: 5, correct: False\n",
      "Edit Percentage: 40.69%\n"
     ]
    }
   ],
   "source": [
    "# Create edited directory\n",
    "edited_dir = challenge_dir / 'edited'\n",
    "\n",
    "# Load edited images, check that they are predicted correctly and calculate edit percentages\n",
    "for original_img_file, edited_img_file in zip(challenge_dir.glob('*.png'), edited_dir.glob('*.png')):\n",
    "    original_img = Image.open(original_img_file)\n",
    "    edited_img = Image.open(edited_img_file)\n",
    "    # Convert the edited image to RGB if it's not\n",
    "    if edited_img.mode != 'RGB':\n",
    "        edited_img = edited_img.convert('RGB')\n",
    "\n",
    "    # Check prediction\n",
    "    img_tensor = to_tensor(edited_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    \n",
    "    print(f'Edited {edited_img_file.name}: Pred: {pred}, Label: {original_img_file.stem[-1]}, correct: {pred == int(original_img_file.stem[-1])}')\n",
    "\n",
    "    # Calculate edit percentage\n",
    "    edit_percentage = calculate_edit_percentage(original_img, edited_img)\n",
    "    print(f'Edit Percentage: {edit_percentage:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
